{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.extmath import safe_sparse_dot, squared_norm\n",
    "from scipy.misc import comb, logsumexp \n",
    "from sklearn.linear_model.logistic import _multinomial_grad_hess\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stratified_bayesian_optimization.lib.util import (\n",
    "    separate_numpy_arrays_in_lists,\n",
    "    wrapper_fit_gp_regression,\n",
    "    get_default_values_kernel,\n",
    "    get_number_parameters_kernel,\n",
    "    combine_vectors,\n",
    "    separate_vector,\n",
    "    wrapper_GPFittingGaussian,\n",
    "    wrapper_objective_acquisition_function,\n",
    "    wrapper_gradient_posterior_mean_gp_model,\n",
    "    wrapper_posterior_mean_gp_model,\n",
    "    wrapper_optimize,\n",
    "    wrapper_sgd,\n",
    "    wrapper_evaluate_gradient_sample_params_gp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stratified_bayesian_optimization.lib.la_functions import (\n",
    "    cholesky,\n",
    "    cho_solve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_samples = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = check_random_state(0)\n",
    "\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.zeros((len(y_train), 10))\n",
    "for i,j in enumerate(y_train):\n",
    "    Y_train[i, int(j)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = np.zeros((len(y_test), 10))\n",
    "for i,j in enumerate(y_test):\n",
    "    Y_test[i, int(j)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(X,Y, w, alpha=0):\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    w = w.reshape(n_classes, -1)\n",
    "    fit_intercept = w.size == (n_classes * (n_features + 1))\n",
    "    old_w = w.copy()\n",
    "    if fit_intercept:\n",
    "        intercept = w[:, -1]\n",
    "        w = w[:, :-1]\n",
    "    else:\n",
    "        intercept = 0\n",
    "    p = safe_sparse_dot(X, w.T)\n",
    "    p += intercept\n",
    "    \n",
    "    p -= logsumexp(p, axis=1)[:, np.newaxis]\n",
    "    loss = -(Y * p).sum()\n",
    "    loss += 0.5 * alpha * squared_norm(w)\n",
    "    p = np.exp(p, p)\n",
    "    return loss, p, w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_loss(X, Y, w, alpha=0):\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    fit_intercept = (w.size == n_classes * (n_features + 1))\n",
    "    grad = np.zeros((n_classes, n_features + bool(fit_intercept)),\n",
    "                    dtype=X.dtype)\n",
    "    loss, p, w = loss_function(X, Y, w,alpha)\n",
    "    diff = (p - Y)\n",
    "    grad[:, :n_features] = safe_sparse_dot(diff.T, X)\n",
    "    grad[:, :n_features] += alpha * w\n",
    "    if fit_intercept:\n",
    "        grad[:, -1] = diff.sum(axis=0)\n",
    "    return loss, grad.ravel(), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(X,Y,w):\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    w = w.reshape(n_classes, -1)\n",
    "    fit_intercept = w.size == (n_classes * (n_features + 1))\n",
    "    old_w = w.copy()\n",
    "    if fit_intercept:\n",
    "        intercept = w[:, -1]\n",
    "        w = w[:, :-1]\n",
    "    else:\n",
    "        intercept = 0\n",
    "    p = safe_sparse_dot(X, w.T)\n",
    "    p += intercept\n",
    "    indices = p.argmax(axis=1)\n",
    "    equal = Y_train.argmax(axis=1) == indices\n",
    "    accuracy = np.sum(equal)\n",
    "    return accuracy / float(Y_train.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_intercept = True\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = 10\n",
    "pairs_tr = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  4.,  7., ...,  2.,  9.,  2.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(momentum=0.9, lr=0.01, batch_size=1001, alpha=4.64711970025, maxepoch=20, adam=False, betas=None, eps=1e-8,\n",
    "       w0=None):\n",
    "    pairs_tr = X_train.shape[0]\n",
    "    if betas is None:\n",
    "        betas = (0.9, 0.999)\n",
    "    if w0 is None:\n",
    "        w0 = np.zeros((n_classes, n_features + int(fit_intercept)),\n",
    "                      order='F', dtype=X.dtype)\n",
    "        np.random.seed(1)\n",
    "        w0 = 0.1 * np.random.randn(n_classes, n_features + int(fit_intercept))\n",
    "        w0 = w0.ravel() \n",
    "    v = np.zeros(len(w0))\n",
    "    num_batches = int(math.ceil(pairs_tr / float(batch_size)))\n",
    "    m0 = np.zeros(len(w0))\n",
    "    v0 = np.zeros(len(w0))\n",
    "    t_ = 0\n",
    "    \n",
    "    loss_vect = []\n",
    "    loss_,g,p_ = grad_loss(X_train,Y_train,w0,alpha)\n",
    "    trajectory_x = []\n",
    "    trajectory_v = []\n",
    "    trajectory_x.append(w0)\n",
    "    trajectory_v.append(v)\n",
    "    loss_vect.append(-1.0 * loss_)\n",
    "    \n",
    "    train_accuracy = []\n",
    "    r = compute_accuracy(X_train,Y_train,w0)\n",
    "    train_accuracy.append(r)\n",
    "    \n",
    "    for epoch in range(0, maxepoch):\n",
    "        shuffled_order = np.arange(pairs_tr)\n",
    "        np.random.shuffle(shuffled_order)\n",
    "        previous = w0.copy()\n",
    "        for batch in range(num_batches):\n",
    "            t_ += 1\n",
    "            next_ = min(batch_size * (batch + 1), pairs_tr)\n",
    "            batch_idx = np.arange(batch_size * batch, next_)\n",
    "            res = batch_size * (batch + 1) - next_\n",
    "            if res > 0:\n",
    "                extra = np.arange(0, res)\n",
    "                batch_idx = np.concatenate((extra, batch_idx))\n",
    "            batch_x = np.array(X_train[shuffled_order[batch_idx],:])  \n",
    "            batch_y = np.array(Y_train[shuffled_order[batch_idx],:])\n",
    "            loss_,g,p_ = grad_loss(batch_x,batch_y,w0,alpha)\n",
    "            old_w = w0.copy()\n",
    "            g = g / float(batch_size)\n",
    "            if not adam:\n",
    "                old_v = v.copy()\n",
    "                v = momentum * v + g\n",
    "                increament = - lr\n",
    "                w0 = w0 - lr * v\n",
    "            else:\n",
    "                increament = 0.0\n",
    "                m0 = betas[0] * m0 + (1 - betas[0]) * g\n",
    "                v0 = betas[1] * v0 + (1 - betas[1]) * ((g) ** 2)\n",
    "                m_1 = m0 / (1 - (betas[0]) ** (t_))\n",
    "                v_1 = v0 / (1 - (betas[1]) ** (t_))\n",
    "                w0 = w0 - lr * m_1 / (np.sqrt(v_1) + eps)\n",
    "        trajectory_x.append(w0)\n",
    "        trajectory_v.append(v)       \n",
    "        loss_,g,p_ = grad_loss(X_train,Y_train,w0,alpha)\n",
    "        loss_vect.append(-1.0 * loss_)\n",
    "        #den_norm = (np.sqrt(np.sum(previous ** 2)))\n",
    "        r = compute_accuracy(X_train,Y_train,w0)\n",
    "        train_accuracy.append(r)\n",
    "\n",
    "    loss, grad_l, p = grad_loss(X_test, Y_test, w0, alpha=0)\n",
    "\n",
    "#    r = compute_accuracy(X_train,Y_train,w0)\n",
    "    \n",
    "    return w0,loss/float(X_test.shape[0]), train_accuracy, train_accuracy, trajectory_x, trajectory_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starting_points={}\n",
    "n_sp = 10\n",
    "for i in range(n_sp): \n",
    "    w0 = np.zeros((n_classes, n_features + int(fit_intercept)),\n",
    "                  order='F', dtype=X.dtype)\n",
    "    np.random.seed(i)\n",
    "    w0 = 10 ** (- i + 4)  * np.random.randn(n_classes, n_features + int(fit_intercept))\n",
    "    w0 = w0.ravel() \n",
    "    starting_points[i] = w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "trajectory_x = {}\n",
    "trajectory_v = {}\n",
    "for i in range(n_sp):\n",
    "    l=sgd(w0=starting_points[i])\n",
    "    results[i] = l[-3]\n",
    "    trajectory_x[i] = l[-2]\n",
    "    trajectory_v[i] = l[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.082400000000000001,\n",
       "  0.082400000000000001,\n",
       "  0.082433333333333331,\n",
       "  0.082433333333333331,\n",
       "  0.082433333333333331,\n",
       "  0.082433333333333331,\n",
       "  0.082500000000000004,\n",
       "  0.082433333333333331,\n",
       "  0.082566666666666663,\n",
       "  0.082633333333333336,\n",
       "  0.082566666666666663,\n",
       "  0.082533333333333334,\n",
       "  0.082633333333333336,\n",
       "  0.082666666666666666,\n",
       "  0.082600000000000007,\n",
       "  0.082766666666666669,\n",
       "  0.082799999999999999,\n",
       "  0.082766666666666669,\n",
       "  0.082799999999999999,\n",
       "  0.082866666666666672,\n",
       "  0.082766666666666669],\n",
       " 1: [0.11623333333333333,\n",
       "  0.11626666666666667,\n",
       "  0.11656666666666667,\n",
       "  0.1167,\n",
       "  0.11696666666666666,\n",
       "  0.11726666666666667,\n",
       "  0.11746666666666666,\n",
       "  0.11763333333333334,\n",
       "  0.11776666666666667,\n",
       "  0.11796666666666666,\n",
       "  0.11806666666666667,\n",
       "  0.1183,\n",
       "  0.11853333333333334,\n",
       "  0.11873333333333333,\n",
       "  0.11899999999999999,\n",
       "  0.11913333333333333,\n",
       "  0.11926666666666667,\n",
       "  0.11953333333333334,\n",
       "  0.11966666666666667,\n",
       "  0.1198,\n",
       "  0.11993333333333334],\n",
       " 2: [0.11559999999999999,\n",
       "  0.11693333333333333,\n",
       "  0.11923333333333333,\n",
       "  0.12139999999999999,\n",
       "  0.1235,\n",
       "  0.1258,\n",
       "  0.12876666666666667,\n",
       "  0.13076666666666667,\n",
       "  0.13306666666666667,\n",
       "  0.13536666666666666,\n",
       "  0.13830000000000001,\n",
       "  0.14050000000000001,\n",
       "  0.14283333333333334,\n",
       "  0.1454,\n",
       "  0.14793333333333333,\n",
       "  0.15126666666666666,\n",
       "  0.15426666666666666,\n",
       "  0.1573,\n",
       "  0.16026666666666667,\n",
       "  0.1636,\n",
       "  0.1666],\n",
       " 3: [0.10616666666666667,\n",
       "  0.1221,\n",
       "  0.14796666666666666,\n",
       "  0.17236666666666667,\n",
       "  0.19900000000000001,\n",
       "  0.22496666666666668,\n",
       "  0.25263333333333332,\n",
       "  0.27779999999999999,\n",
       "  0.30316666666666664,\n",
       "  0.32629999999999998,\n",
       "  0.35039999999999999,\n",
       "  0.37326666666666669,\n",
       "  0.39336666666666664,\n",
       "  0.4133,\n",
       "  0.43253333333333333,\n",
       "  0.45169999999999999,\n",
       "  0.47110000000000002,\n",
       "  0.48780000000000001,\n",
       "  0.50223333333333331,\n",
       "  0.51703333333333334,\n",
       "  0.52993333333333337],\n",
       " 4: [0.067400000000000002,\n",
       "  0.29870000000000002,\n",
       "  0.53346666666666664,\n",
       "  0.63560000000000005,\n",
       "  0.68676666666666664,\n",
       "  0.71973333333333334,\n",
       "  0.74239999999999995,\n",
       "  0.76036666666666664,\n",
       "  0.77393333333333336,\n",
       "  0.78643333333333332,\n",
       "  0.79536666666666667,\n",
       "  0.80430000000000001,\n",
       "  0.81089999999999995,\n",
       "  0.81703333333333328,\n",
       "  0.82253333333333334,\n",
       "  0.82706666666666662,\n",
       "  0.83076666666666665,\n",
       "  0.83430000000000004,\n",
       "  0.83723333333333338,\n",
       "  0.84023333333333339,\n",
       "  0.84273333333333333],\n",
       " 5: [0.064166666666666664,\n",
       "  0.80846666666666667,\n",
       "  0.86373333333333335,\n",
       "  0.87886666666666668,\n",
       "  0.8874333333333333,\n",
       "  0.89280000000000004,\n",
       "  0.89839999999999998,\n",
       "  0.90186666666666671,\n",
       "  0.90546666666666664,\n",
       "  0.90773333333333328,\n",
       "  0.90980000000000005,\n",
       "  0.91186666666666671,\n",
       "  0.91333333333333333,\n",
       "  0.91543333333333332,\n",
       "  0.91723333333333334,\n",
       "  0.9180666666666667,\n",
       "  0.91900000000000004,\n",
       "  0.91993333333333338,\n",
       "  0.92100000000000004,\n",
       "  0.92116666666666669,\n",
       "  0.92193333333333338],\n",
       " 6: [0.074499999999999997,\n",
       "  0.87156666666666671,\n",
       "  0.89476666666666671,\n",
       "  0.90310000000000001,\n",
       "  0.90849999999999997,\n",
       "  0.9116333333333333,\n",
       "  0.91413333333333335,\n",
       "  0.91620000000000001,\n",
       "  0.91826666666666668,\n",
       "  0.91900000000000004,\n",
       "  0.92026666666666668,\n",
       "  0.92116666666666669,\n",
       "  0.92183333333333328,\n",
       "  0.92210000000000003,\n",
       "  0.92336666666666667,\n",
       "  0.92376666666666662,\n",
       "  0.92423333333333335,\n",
       "  0.92469999999999997,\n",
       "  0.92556666666666665,\n",
       "  0.92593333333333339,\n",
       "  0.92616666666666669],\n",
       " 7: [0.067966666666666661,\n",
       "  0.87280000000000002,\n",
       "  0.89529999999999998,\n",
       "  0.90333333333333332,\n",
       "  0.90890000000000004,\n",
       "  0.91186666666666671,\n",
       "  0.91483333333333339,\n",
       "  0.91646666666666665,\n",
       "  0.91830000000000001,\n",
       "  0.91903333333333337,\n",
       "  0.92076666666666662,\n",
       "  0.92130000000000001,\n",
       "  0.92200000000000004,\n",
       "  0.92256666666666665,\n",
       "  0.92346666666666666,\n",
       "  0.92393333333333338,\n",
       "  0.92476666666666663,\n",
       "  0.92496666666666671,\n",
       "  0.92546666666666666,\n",
       "  0.92589999999999995,\n",
       "  0.92626666666666668],\n",
       " 8: [0.11459999999999999,\n",
       "  0.87216666666666665,\n",
       "  0.89553333333333329,\n",
       "  0.90343333333333331,\n",
       "  0.90890000000000004,\n",
       "  0.91169999999999995,\n",
       "  0.91466666666666663,\n",
       "  0.91623333333333334,\n",
       "  0.91773333333333329,\n",
       "  0.91956666666666664,\n",
       "  0.91986666666666672,\n",
       "  0.92086666666666661,\n",
       "  0.92176666666666662,\n",
       "  0.92293333333333338,\n",
       "  0.92336666666666667,\n",
       "  0.92379999999999995,\n",
       "  0.92456666666666665,\n",
       "  0.92520000000000002,\n",
       "  0.92583333333333329,\n",
       "  0.92643333333333333,\n",
       "  0.92626666666666668],\n",
       " 9: [0.11043333333333333,\n",
       "  0.87276666666666669,\n",
       "  0.89506666666666668,\n",
       "  0.90376666666666672,\n",
       "  0.90866666666666662,\n",
       "  0.91180000000000005,\n",
       "  0.91459999999999997,\n",
       "  0.91616666666666668,\n",
       "  0.91779999999999995,\n",
       "  0.91879999999999995,\n",
       "  0.92036666666666667,\n",
       "  0.92136666666666667,\n",
       "  0.92190000000000005,\n",
       "  0.92290000000000005,\n",
       "  0.92346666666666666,\n",
       "  0.92410000000000003,\n",
       "  0.92463333333333331,\n",
       "  0.92503333333333337,\n",
       "  0.92556666666666665,\n",
       "  0.92583333333333329,\n",
       "  0.92636666666666667]}"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11a1f9310>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCrJ2AwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f7cfb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "x_lim = len(results[0])\n",
    "points = range(x_lim)\n",
    "for i in range(3,n_sp):\n",
    "    plt.plot(points, results[i][0:x_lim],label=str(i))\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylabel('Objective function')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11f704090>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRHL+Bw32cJQlwIr1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121aa7f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "x_lim = len(results[0])\n",
    "points = range(x_lim)\n",
    "for i in range(3,n_sp):\n",
    "    plt.plot(points, [np.exp(-(results[i][-1] - t)) for t in results[i][0:x_lim]],label=str(i))\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylabel('Objective function')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = [2, 5, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_results = {}\n",
    "for j in points:\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [0.11559999999999999, 0.11693333333333333, 0.11923333333333333],\n",
       " 5: [0.064166666666666664, 0.80846666666666667, 0.86373333333333335],\n",
       " 9: [0.11043333333333333, 0.87276666666666669, 0.89506666666666668]}"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_results = {}\n",
    "for j in points:\n",
    "    best_results[j] = np.max(raw_results[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_iterations = {}\n",
    "for j in points:\n",
    "    current_iterations[j] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "differences = {}\n",
    "for j in points:\n",
    "    differences[j] = []\n",
    "    for i in range(1, len(raw_results[j])):\n",
    "        diff = raw_results[j][i] - raw_results[j][i-1]\n",
    "        differences[j].append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points_differences = {}\n",
    "for j in points:\n",
    "    points_differences[j] = []\n",
    "    for i in range(1, len(raw_results[j])):\n",
    "        points_differences[j].append((i, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = {}\n",
    "for j in points:\n",
    "    training_data[j] = {}\n",
    "    training_data[j]['points'] = [[2.], [3.]]\n",
    "    training_data[j]['evaluations'] = list(differences[j])\n",
    "    training_data[j]['var_noise'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/Users/saultoscano/Documents/research/GitHub/stratified_bayesian_optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stratified_bayesian_optimization.lib.constant import (\n",
    "    SCALED_KERNEL,\n",
    "    MATERN52_NAME,\n",
    "    TASKS_KERNEL_NAME,\n",
    "    PRODUCT_KERNELS_SEPARABLE,\n",
    "    UNIFORM_FINITE,\n",
    "    SBO_METHOD,\n",
    "    EI_METHOD,\n",
    "    DOGLEG,\n",
    "    MULTI_TASK_METHOD,\n",
    "    EXPONENTIAL,\n",
    "    SCALED_KERNEL,\n",
    "    GAMMA,\n",
    "    WEIGHTED_UNIFORM_FINITE,\n",
    "    SDE_METHOD,\n",
    "    LBFGS_NAME,\n",
    "    ORNSTEIN_KERNEL,\n",
    ")\n",
    "from stratified_bayesian_optimization.services.gp_fitting import GPFittingService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective_function(x):\n",
    "    return [np.sum(x[1:3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specs = {}\n",
    "for i in points:\n",
    "    spec = {\n",
    "                'name_model': 'gp_fitting_gaussian',\n",
    "                'problem_name': 'logistic',\n",
    "                'type_kernel': [ORNSTEIN_KERNEL],\n",
    "                'dimensions': [1],\n",
    "                'bounds_domain': [[1,100]],\n",
    "                'type_bounds': [0],\n",
    "                'n_training': 10,\n",
    "                'noise': False,\n",
    "                'training_data': training_data[i],\n",
    "                'points': None,\n",
    "                'training_name': None,\n",
    "                'mle': False,\n",
    "                'thinning': 10,\n",
    "                'n_burning': 500,\n",
    "                'max_steps_out': 1000,\n",
    "                'n_samples': 0,\n",
    "                'random_seed': 1,\n",
    "                'kernel_values': None,\n",
    "                'mean_value': None,\n",
    "                'var_noise_value': None,\n",
    "                'cache': True,\n",
    "                'same_correlation': True,\n",
    "                'use_only_training_points': True,\n",
    "                'optimization_method': 'SBO',\n",
    "                'n_samples_parameters': 10,\n",
    "                'parallel_training': False,\n",
    "                'simplex_domain': None,\n",
    "                'objective_function': objective_function,\n",
    "                'define_samplers': False\n",
    "    }\n",
    "    specs[i] = spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_gp_model_from_spec(spec, st_point):\n",
    "    model = GPFittingService.from_dict(spec)\n",
    "    model.dimension_parameters -= 2\n",
    "    model.best_results = best_results[st_point]\n",
    "    model.current_iterations = current_iterations[st_point]\n",
    "    model.raw_results = list(raw_results[st_point])\n",
    "    model.data['points'] = list(points_differences[st_point])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:stratified_bayesian_optimization.util.json_file:Loading data/gp_models/logistic/gp_GPFittingGaussian_logistic_Ornstein_kernel_default_training_data_10_points_rs_1.json\n",
      "INFO:stratified_bayesian_optimization.services.gp_fitting:Training GPFittingGaussian\n",
      "INFO:stratified_bayesian_optimization.util.json_file:Loading data/gp_models/logistic/gp_GPFittingGaussian_logistic_Ornstein_kernel_default_training_data_10_points_rs_1.json\n",
      "INFO:stratified_bayesian_optimization.services.gp_fitting:Training GPFittingGaussian\n",
      "INFO:stratified_bayesian_optimization.util.json_file:Loading data/gp_models/logistic/gp_GPFittingGaussian_logistic_Ornstein_kernel_default_training_data_10_points_rs_1.json\n",
      "INFO:stratified_bayesian_optimization.services.gp_fitting:Training GPFittingGaussian\n"
     ]
    }
   ],
   "source": [
    "gp_models_no_mean = {}\n",
    "for i in points:\n",
    "    gp_models_no_mean[i] = create_gp_model_from_spec(specs[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covariance_diff_kernel(self, X, params):\n",
    "    f = lambda x: np.sqrt(float(x))\n",
    "    raw_x = []\n",
    "    raw_x.append([X[0][0]])\n",
    "    for t in X:\n",
    "        raw_x.append([t[1]])\n",
    "    raw_x = np.array(raw_x)\n",
    "    raw_cov = self.kernel.evaluate_cov_defined_by_params(params, raw_x, 1)\n",
    "    n= len(X)\n",
    "    cov_mat = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        diff = (raw_cov[i, i] / f(i + 1))  + (raw_cov[i+1, i+1] / f(i + 2))\n",
    "        diff -= (2.0 * raw_cov[i, i+1] / f(i+2))\n",
    "        cov_mat[i, i] = diff\n",
    "        for j in range(i + 1, n):\n",
    "            diff = (raw_cov[i, j] / f(j+1)) + (raw_cov[i+1, j+1] / f(j+2))\n",
    "            diff -= ((raw_cov[i, j+1] / f(j+2)) + (raw_cov[i+1,j] / f(j+1)))\n",
    "            cov_mat[i,j]= diff\n",
    "            cov_mat[j,i]= diff\n",
    "    return cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(self, parameters_kernel):\n",
    "    \"\"\"\n",
    "    GP log likelihood: y(x) ~ f(x) + epsilon, where epsilon(x) are iid N(0,var_noise), and\n",
    "    f(x) ~ GP(mean, cov)\n",
    "\n",
    "    :param var_noise: (float) variance of the noise\n",
    "    :param mean: (float)\n",
    "    :param parameters_kernel: np.array(k), The order of the parameters is given in the\n",
    "        definition of the class kernel.\n",
    "    :return: float\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    X_data = self.data['points']\n",
    "    n = len(X_data)\n",
    "    cov = covariance_diff_kernel(self, X_data, parameters_kernel)\n",
    "    chol = cholesky(cov,  max_tries=7)\n",
    "   \n",
    "    y_unbiased = self.data['evaluations']\n",
    "    solve = cho_solve(chol, y_unbiased)\n",
    "\n",
    "    return -np.sum(np.log(np.diag(chol))) - 0.5 * np.dot(y_unbiased, solve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.848152852156041"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood(gp_model,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_prob(parameters, self):\n",
    "        lp = 0.0\n",
    "        parameters_model = self.get_parameters_model[2:]\n",
    "        index = 0\n",
    "\n",
    "        for parameter in parameters_model:\n",
    "            dimension = parameter.dimension\n",
    "            lp += parameter.log_prior(parameters[index: index + dimension])\n",
    "\n",
    "            index += dimension\n",
    "        if not np.isinf(lp):\n",
    "            lp += log_likelihood(self, parameters)\n",
    "            \n",
    "        return lp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30.58876667])"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob(params, gp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_parameters(self, n_samples, start_point=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    Sample parameters of the model from the posterior without considering burning.\n",
    "\n",
    "    :param n_samples: (int)\n",
    "    :param start_point: np.array(n_parameters)\n",
    "    :param random_seed: int\n",
    "\n",
    "    :return: n_samples * [np.array(float)]\n",
    "    \"\"\"\n",
    "\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    if start_point is None:\n",
    "        start_point = self.samples_parameters[-1]\n",
    "\n",
    "\n",
    "    n_samples *= (self.thinning + 1)\n",
    "    n_samples = int(n_samples)\n",
    "\n",
    "    if len(self.slice_samplers) == 1:\n",
    "        for sample in xrange(n_samples):\n",
    "            start_point_ = None\n",
    "            n_try = 0\n",
    "            points = start_point\n",
    "           # points = separate_vector(start_point, self.length_scale_indexes)\n",
    "            while start_point_ is None and n_try < 10:\n",
    "                try:\n",
    "                    start_point_ = \\\n",
    "                        self.slice_samplers[0].slice_sample(points, None, *(self, ))\n",
    "                except Exception as e:\n",
    "                    n_try += 1\n",
    "                    start_point_ = None\n",
    "            if start_point_ is None:\n",
    "                logger.info('program failed to compute a sample of the parameters')\n",
    "                sys.exit(1)\n",
    "            start_point = start_point_\n",
    "            samples.append(start_point)\n",
    "        samples_return = samples[::self.thinning + 1]\n",
    "        self.samples_parameters += samples_return\n",
    "        return samples_return\n",
    "\n",
    "    for sample in xrange(n_samples):\n",
    "        points = separate_vector(start_point, self.length_scale_indexes)\n",
    "        for index, slice in enumerate(self.slice_samplers):\n",
    "            new_point_ = None\n",
    "            n_try = 0\n",
    "            while new_point_ is None and n_try < 10:\n",
    "                try:\n",
    "                    new_point_ = \\\n",
    "                        slice.slice_sample(points[1 - index], points[index], *(self, ))\n",
    "                except Exception as e:\n",
    "                    n_try += 1\n",
    "                    new_point_ = None\n",
    "            if new_point_ is None:\n",
    "                logger.info('program failed to compute a sample of the parameters')\n",
    "                sys.exit(1)\n",
    "            points[1 - index] = new_point_\n",
    "        start_point = combine_vectors(points[0], points[1], self.length_scale_indexes)\n",
    "        samples.append(start_point)\n",
    "    samples_return = samples[::self.thinning + 1]\n",
    "    self.samples_parameters += samples_return\n",
    "\n",
    "    return samples_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stratified_bayesian_optimization.samplers.slice_sampling import SliceSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_samplers(self):\n",
    "    \"\"\"\n",
    "    Defines the samplers of the parameters of the model.\n",
    "    We assume that we only have one set of length scale parameters.\n",
    "    \"\"\"\n",
    "    self.slice_samplers = []\n",
    "    self.samples_parameters = []\n",
    "    self.start_point_sampler = []\n",
    "    if self.length_scale_indexes is None:\n",
    "        ignore_index = None\n",
    "\n",
    "        slice_parameters = {\n",
    "            'max_steps_out': self.max_steps_out,\n",
    "            'component_wise': True,\n",
    "        }\n",
    "        self.slice_samplers.append(SliceSampling(\n",
    "            log_prob, range(self.dimension_parameters),  ignore_index=ignore_index,\n",
    "            **slice_parameters))\n",
    "    else:\n",
    "        slice_parameters = {\n",
    "            'max_steps_out': self.max_steps_out,\n",
    "            'component_wise': False,\n",
    "        }\n",
    "        indexes = [i for i in range(self.dimension_parameters) if i not in\n",
    "                   self.length_scale_indexes]\n",
    "        ignore_index = None\n",
    "        if not self.noise or self.data.get('var_noise') is not None:\n",
    "            ignore_index = [0, 1]\n",
    "\n",
    "        if ORNSTEIN_KERNEL in self.type_kernel:\n",
    "            if ignore_index is None:\n",
    "                ignore_index = []\n",
    "            ignore_index += [2]\n",
    "\n",
    "        if len(indexes) != len(ignore_index):\n",
    "            self.slice_samplers.append(\n",
    "                SliceSampling(\n",
    "                    log_prob, indexes, ignore_index=ignore_index, **slice_parameters))\n",
    "\n",
    "        slice_parameters['component_wise'] = True\n",
    "        self.slice_samplers.append(SliceSampling(wrapper_log_prob, self.length_scale_indexes,\n",
    "                                                 **slice_parameters))\n",
    "\n",
    "    if self.start_point_sampler is not None and len(self.start_point_sampler) > 0:\n",
    "        if len(self.samples_parameters) == 0:\n",
    "            self.samples_parameters.append(np.array(self.start_point_sampler))\n",
    "    else:\n",
    "        self.samples_parameters = []\n",
    "        z = self.get_value_parameters_model\n",
    "        z = z[2:]\n",
    "        self.samples_parameters.append(z)\n",
    "        if self.n_burning > 0:\n",
    "            parameters = sample_parameters(self, float(self.n_burning) / (self.thinning + 1))\n",
    "            self.samples_parameters = []\n",
    "            self.samples_parameters.append(parameters[-1])\n",
    "            self.start_point_sampler = parameters[-1]\n",
    "        else:\n",
    "            self.start_point_sampler = self.get_value_parameters_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in points:\n",
    "    print i\n",
    "    set_samplers(gp_models_no_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_samplers(gp_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov_diff_point(self, params, x, X):\n",
    "    f = lambda x: np.sqrt(float(x))\n",
    "    z = np.array([x])\n",
    "    \n",
    "    raw_x = []\n",
    "    raw_x.append([X[0][0]])\n",
    "    for t in X:\n",
    "        raw_x.append([t[1]])\n",
    "        \n",
    "    raw_x = np.array(raw_x)\n",
    "    raw_cov = self.kernel.evaluate_cross_cov_defined_by_params(params, z, raw_x, 1)\n",
    "    \n",
    "    cov = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        cov[i] = (raw_cov[0, i] - raw_cov[0, i+1]) / f(x[0])\n",
    "        \n",
    "    return cov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_posterior_params(self, params):\n",
    "    ## Mean is zero. We may need to change when including mean\n",
    "    ##Compute posterior parameters of f(x*)\n",
    "    f = lambda x: np.sqrt(float(x))\n",
    "    current_point = [self.current_iterations]\n",
    "    X_data = self.data['points']\n",
    "    vector_ = cov_diff_point(self, params, current_point, X_data)\n",
    "    \n",
    "    cov = covariance_diff_kernel(self, X_data, params)\n",
    "    chol = cholesky(cov,  max_tries=7)\n",
    " \n",
    "    y_unbiased = self.data['evaluations']\n",
    "    solve = cho_solve(chol, y_unbiased)\n",
    "    \n",
    "    part_2 = cho_solve(chol, vector_)\n",
    "    \n",
    "\n",
    "    mean =  self.raw_results[-1] + np.dot(part_2, solve)\n",
    "    \n",
    "    raw_cov = self.kernel.evaluate_cov_defined_by_params(params, np.array([current_point]), 1) / f(current_point[0]) \n",
    "    \n",
    "    var = raw_cov - np.dot(part_2, part_2)\n",
    "    \n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean, cov = compute_posterior_params(gp_model, gp_model.samples_parameters[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_posterior_params_marginalize(self, n_samples=10, burning_parameters=True):\n",
    "    \n",
    "    if burning_parameters:\n",
    "        parameters = sample_parameters(self, float(self.n_burning) / (self.thinning + 1))\n",
    "        self.samples_parameters = []\n",
    "        self.samples_parameters.append(parameters[-1])\n",
    "        self.start_point_sampler = parameters[-1]\n",
    "        \n",
    "    parameters = sample_parameters(self, n_samples)\n",
    "    \n",
    "    means = []\n",
    "    covs = []\n",
    "    for param in parameters:\n",
    "        mean, cov = compute_posterior_params(self, param)\n",
    "        means.append(mean)\n",
    "        covs.append(cov)\n",
    "        \n",
    "    mean = np.mean(means)\n",
    "    std = np.sqrt(np.mean(covs))\n",
    "    \n",
    "    ci = [mean-1.96 * std, mean+ 1.96*std]\n",
    "    \n",
    "    return mean, std, ci\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_results = {}\n",
    "for j in points:\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_observations(self, point, y, previous_y):\n",
    "    self.current_iterations = point\n",
    "    self.raw_results.append(y)\n",
    "    self.best_results = max(self.best_results, y)\n",
    "    self.data['points'].append((point-1, point))\n",
    "    self.data['evaluations'] = np.concatenate((self.data['evaluations'],[(y - previous_y)]))\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(self, st_point, iterations=21, start=3):\n",
    "    means = []\n",
    "    cis = []\n",
    "    \n",
    "    \n",
    "    mean, std, ci = compute_posterior_params_marginalize(self)\n",
    "    means.append(mean)\n",
    "    cis.append(ci)\n",
    "    \n",
    "    for i in range(start, iterations):\n",
    "        print (i)\n",
    "        self = add_observations(self, i + 1, results[st_point][i], results[st_point][i-1])\n",
    "        mean, std, ci = compute_posterior_params_marginalize(self)\n",
    "        means.append(mean)\n",
    "        cis.append(ci)\n",
    "    return means, cis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_means(means, cis, original_value, start=3):\n",
    "    plt.figure()\n",
    "    x_lim = len(means)\n",
    "    points = range(start, x_lim + start)\n",
    "    plt.plot(points, means,'b')\n",
    "    plt.plot(points, len(points) * [original_value], 'r')\n",
    "    plt.plot(points, [t[0] for t in cis],'g-')\n",
    "    plt.plot(points, [t[1] for t in cis],'g-')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel('Objective function')\n",
    "    plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWOIwxhjTLP8f/Fgmm7fTtNAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12272df90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_means(means,cis, results[2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "her\n",
      "0.121199058516 [-1.1536002309709898, 1.3959983480028377]\n",
      "4\n",
      "her\n",
      "0.123305994765 [-0.99538674449386155, 1.2419987340237899]\n",
      "5\n",
      "her\n",
      "0.125476150574 [-0.80773621272229534, 1.0586885138708522]\n",
      "6\n",
      "her\n",
      "0.128315711901 [-0.65154897541101298, 0.90818039921204996]\n",
      "7\n",
      "her\n",
      "0.130098409824 [-0.51277902080683924, 0.77297584045392642]\n",
      "8\n",
      "her\n",
      "0.13061562731 [-0.53075673018900926, 0.7919879848089002]\n",
      "9\n",
      "her\n",
      "0.134485797447 [-0.44517562422967183, 0.71414721912366019]\n",
      "10\n",
      "her\n",
      "0.123605564096 [-0.43028880383531459, 0.67749993202775438]\n",
      "11\n",
      "her\n",
      "0.136900328332 [-0.33535208160718744, 0.60915273827170302]\n",
      "12\n",
      "her\n",
      "0.139009059471 [-0.38733752283141298, 0.66535564177419004]\n",
      "13\n",
      "her\n",
      "0.142027284875 [-0.27451124217550377, 0.55856581192560983]\n",
      "14\n",
      "her\n",
      "0.0922258150234 [-0.279783943435298, 0.46423557348212929]\n",
      "15\n",
      "her\n",
      "0.139593063102 [-0.24230122386485387, 0.52148735006827107]\n",
      "16\n",
      "her\n",
      "0.142045320844 [-0.25812967719953089, 0.54222031888762756]\n",
      "17\n",
      "her\n",
      "0.147654833139 [-0.2294970812963249, 0.52480674757476398]\n",
      "18\n",
      "her\n",
      "0.142476627945 [-0.2012771910570173, 0.48623044694646139]\n",
      "19\n",
      "her\n",
      "0.113387373381 [-0.20130567255385662, 0.42808041931586421]\n",
      "20\n",
      "her\n",
      "0.0609530594209 [-0.24111684516348897, 0.36302296400526313]\n"
     ]
    }
   ],
   "source": [
    "means, cis = accuracy(gp_model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric mean!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, a, b):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    \n",
    "    return a * x + b\n",
    "\n",
    "def grad_linear(x,a,b):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "  \n",
    "    grad = np.zeros(2)\n",
    "    \n",
    "    grad[0] = x\n",
    "    grad[1] = 1.0\n",
    " \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vapor_pressure(x, a, b, c):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    \n",
    "    return np.exp(a + (b / x) + c * np.log(x))\n",
    "\n",
    "def grad_vapor_pressure(x,a,b,c):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    \n",
    "    grad = np.zeros(3)\n",
    "    value = vapor_pressure(x, a, b, c)\n",
    "    \n",
    "    grad[0] = value\n",
    "    grad[1] = value * (1.0 / x)\n",
    "    grad[2] = value * np.log(x)\n",
    "        \n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pow_3(x, a, c, alpha):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    alpha = float(alpha)\n",
    "    c = float(c) \n",
    "    \n",
    "    val = c - a * (np.power(x, -1.0 * alpha))\n",
    "    \n",
    "\n",
    "    \n",
    "    return val\n",
    "\n",
    "def grad_pow_3(x, a, c, alpha):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    alpha = float(alpha)\n",
    "    c = float(c) \n",
    "    \n",
    "    grad = np.zeros(3)\n",
    "    grad[0] = -np.power(x, -1.0 * alpha)\n",
    "    grad[1] = 1.0\n",
    "    grad[2] = a * (np.power(x, -1.0 * alpha)) * np.log(x)\n",
    "    \n",
    "\n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d={'a': 1.1098132161448053, 'alpha': 0.57295007091643357, 'c': 0.261028854159291}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_log_linear(x, a, b):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "\n",
    "    return np.log(a * np.log(x) + b)\n",
    "\n",
    "def grad_log_log_linear(x, a, b):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    \n",
    "    grad = np.zeros(2)\n",
    "    grad[0] = np.log(x) / (a * np.log(x) + b)\n",
    "    grad[1] = 1.0 / (a * np.log(x) + b)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hill_3(x, eta, k, theta, alpha):\n",
    "    x = float(x)\n",
    "    eta = float(eta)\n",
    "    k = float(k)\n",
    "    theta = float(theta)\n",
    "    alpha = float(alpha)\n",
    "    \n",
    "    return alpha + (theta * np.power(x, eta)) / (np.power(k, eta) + np.power(x, eta))\n",
    "\n",
    "def grad_hill_3(x, eta, k, theta, alpha):\n",
    "    x = float(x)\n",
    "    eta = float(eta)\n",
    "    k = float(k)\n",
    "    theta = float(theta)\n",
    "    alpha = float(alpha)\n",
    "    \n",
    "    grad = np.zeros(4)\n",
    "    grad[0] = theta * (np.power(k, eta) * (np.power(k, eta) + np.power(x, eta)) * np.log(x) - np.power(x, eta) * (np.power(k, eta) * np.log(k) + np.power(x, eta) * np.log(x))) / ((np.power(k, eta) + np.power(x, eta)) ** 2)\n",
    "    grad[1] = - (theta * np.power(x, eta) * np.power(k, eta-1) * eta) / ((np.power(k, eta) + np.power(x, eta)) ** 2)\n",
    "    grad[2] = (np.power(x, eta)) / (np.power(k, eta) + np.power(x, eta))\n",
    "    grad[3] = 1.0\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_power(x, a, b ,c):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    \n",
    "    return a / (1.0 + np.power((x / np.exp(b)), c))\n",
    "\n",
    "def grad_log_power(x, a, b ,c):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    \n",
    "    grad = np.zeros(3)\n",
    "    \n",
    "    grad[0] = 1.0 / (1.0 + np.power((x / np.exp(b)), c))\n",
    "    grad[1] = (a * np.power((x / np.exp(b)), c) *  c) / ((1.0 + np.power((x / np.exp(b)), c)) ** 2)\n",
    "    grad[2] = (- a * np.power((x / np.exp(b)), c) *  (np.log(x) - b)) / ((1.0 + np.power((x / np.exp(b)), c)) ** 2)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pow_4(x, a, b ,c, alpha):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    alpha = float(alpha)\n",
    "    \n",
    "    return c - np.power(a * x + b, -1.0*alpha)\n",
    "\n",
    "def grad_pow_4(x, a, b ,c, alpha):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    alpha = float(alpha)\n",
    "    \n",
    "    grad = np.zeros(4)\n",
    "    \n",
    "    grad[0] = alpha * x * np.power(a * x + b, -1.0*alpha) / (a * x + b)\n",
    "    grad[1] = alpha * np.power(a * x + b, -1.0*alpha) / (a * x + b)\n",
    "    grad[2] = 1.0\n",
    "    grad[3] = np.power(a * x + b, -1.0*alpha) * np.log(a * x + b)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d={'a': 135.78110609631275, 'alpha': 0.14096454463848812, 'c': 1.0000009999999999, 'b': -444.54248022781883}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saultoscano/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: RuntimeWarning: invalid value encountered in power\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(135.0-444,-1.0*1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saultoscano/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: RuntimeWarning: invalid value encountered in power\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_4(1,**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mmf(x, alpha, beta, k, delta):\n",
    "    x = float(x)\n",
    "    alpha = float(alpha)\n",
    "    beta = float(beta)\n",
    "    k = float(k)\n",
    "    delta = float(delta)\n",
    "    \n",
    "    return alpha - ((alpha - beta) / (1.0 + np.power(k*x,delta)))\n",
    "\n",
    "def grad_mmf(x, alpha, beta, k, delta):\n",
    "    x = float(x)\n",
    "    alpha = float(alpha)\n",
    "    beta = float(beta)\n",
    "    k = float(k)\n",
    "    delta = float(delta)\n",
    "    \n",
    "    grad = np.zeros(4)\n",
    "    \n",
    "    grad[0] = 1.0 - 1.0 / ((1.0 + np.power(k*x,delta)))\n",
    "    grad[1] = 1.0 /  ((1.0 + np.power(k*x,delta)))\n",
    "    grad[2] = (alpha - beta) * np.power(k*x,delta) * delta * (1.0 / k) / (((1.0 + np.power(k*x,delta)))**2)\n",
    "    grad[3] = (alpha - beta) * np.power(k*x,delta) * np.log(k*x)/ (((1.0 + np.power(k*x,delta)))**2)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_4(x, a, b, c, alpha):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    alpha = float(alpha)\n",
    "    \n",
    "    \n",
    "    return c - np.exp(b - a * np.power(x, alpha))\n",
    "    \n",
    "def grad_exp_4(x, a, b, c, alpha):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    c = float(c)\n",
    "    alpha = float(alpha)\n",
    "    \n",
    "    grad = np.zeros(4)\n",
    "    \n",
    "    grad[0] = - np.exp(b - a * np.power(x, alpha)) * (- np.power(x, alpha))\n",
    "    grad[1] = - np.exp(b - a * np.power(x, alpha))\n",
    "    grad[2] = 1.0\n",
    "    grad[3] =  a * np.power(x, alpha) * np.log(x) * np.exp(b - a * np.power(x, alpha))\n",
    "    \n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d={'a': 0.6848095987297137, 'alpha': 0.4465773916156186, 'c': 0.2, 'b': -0.63425983343071968}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06738400518633475"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_4(1,**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def janoschek(x, alpha, beta, k, delta):\n",
    "    x = float(x)\n",
    "    alpha = float(alpha)\n",
    "    beta = float(beta)\n",
    "    k = float(k)\n",
    "    delta = float(delta)\n",
    "    \n",
    "    return alpha - (alpha - beta) * np.exp(-k * np.power(x, delta))\n",
    "    \n",
    "def grad_janoschek(x, alpha, beta, k, delta):\n",
    "    x = float(x)\n",
    "    alpha = float(alpha)\n",
    "    beta = float(beta)\n",
    "    k = float(k)\n",
    "    delta = float(delta)\n",
    "    \n",
    "    grad = np.zeros(4)\n",
    "    \n",
    "    grad[0] = 1.0 - np.exp(-k * np.power(x, delta))\n",
    "    grad[1] = np.exp(-k * np.power(x, delta))\n",
    "    grad[2] =  (alpha - beta) * np.exp(-k * np.power(x, delta)) * np.power(x, delta)\n",
    "    grad[3] = (alpha - beta) * np.exp(-k * np.power(x, delta)) * k * np.power(x, delta) * np.log(x)\n",
    "    \n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weibull(x, alpha, beta, k, delta ):\n",
    "    x = float(x)\n",
    "    alpha = float(alpha)\n",
    "    beta = float(beta)\n",
    "    k = float(k)\n",
    "    delta = float(delta)\n",
    "    \n",
    "    return alpha - (alpha - beta) * np.exp(- np.power(k * x, delta))\n",
    "   \n",
    "    \n",
    "def grad_weibull(x, alpha, beta, k, delta ):\n",
    "    x = float(x)\n",
    "    alpha = float(alpha)\n",
    "    beta = float(beta)\n",
    "    k = float(k)\n",
    "    delta = float(delta)\n",
    "    \n",
    "    grad = np.zeros(4)\n",
    "    \n",
    "    grad[0] = 1.0 - np.exp(- np.power(k * x, delta))\n",
    "    grad[1] = np.exp(- np.power(k * x, delta))\n",
    "    grad[2] = (alpha - beta) * np.exp(- np.power(k * x, delta)) * np.power(k * x, delta) * delta * (1.0 / k)\n",
    "    grad[3] = (alpha - beta) * np.exp(- np.power(k * x, delta)) * np.power(k * x, delta) * np.log(k * x)\n",
    "    \n",
    "    return grad\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ilog2(x, a ,c):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    c = float(c)\n",
    "    return c - (a / np.log(x + 1))\n",
    "\n",
    "def grad_ilog2(x, a ,c):\n",
    "    x = float(x)\n",
    "    a = float(a)\n",
    "    c = float(c)\n",
    "    \n",
    "    grad = np.zeros(2)\n",
    "    grad[0] = -1.0 / (np.log(x+1))\n",
    "    grad[1] = 1.0\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.56099183080536819"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d={'a': 0.85966792742057863, 'c': 0.67924682489559396}\n",
    "ilog2(1,**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41588830833596718"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.6 *np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "functions = {\n",
    "    'linear': linear,\n",
    "    'ilog2': ilog2,\n",
    "    'weibull': weibull,\n",
    "    'janoschek': janoschek,\n",
    "    'exp_4': exp_4,\n",
    "    'mmf': mmf,\n",
    "    'pow_4': pow_4,\n",
    "    'log_power': log_power,\n",
    "    'hill_3': hill_3,\n",
    "    'log_log_linear': log_log_linear,\n",
    "    'pow_3': pow_3,\n",
    "    'vapor_pressure': vapor_pressure,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients_functions = {\n",
    "    'linear': grad_linear,\n",
    "    'ilog2': grad_ilog2,\n",
    "    'weibull': grad_weibull,\n",
    "    'janoschek': grad_janoschek,\n",
    "    'exp_4': grad_exp_4,\n",
    "    'mmf': grad_mmf,\n",
    "    'pow_4': grad_pow_4,\n",
    "    'log_power': grad_log_power,\n",
    "    'hill_3': grad_hill_3,\n",
    "    'log_log_linear': grad_log_log_linear,\n",
    "    'pow_3': grad_pow_3,\n",
    "    'vapor_pressure': grad_vapor_pressure,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear',\n",
       " 'hill_3',\n",
       " 'vapor_pressure',\n",
       " 'mmf',\n",
       " 'janoschek',\n",
       " 'log_power',\n",
       " 'log_log_linear',\n",
       " 'weibull',\n",
       " 'exp_4',\n",
       " 'pow_4',\n",
       " 'pow_3',\n",
       " 'ilog2']"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_functions = functions.keys()\n",
    "list_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_functions = {\n",
    "    'linear': ['a', 'b'],\n",
    "    'ilog2': ['a', 'c'],\n",
    "    'weibull': ['alpha', 'beta', 'k', 'delta'],\n",
    "    'janoschek': ['alpha', 'beta', 'k', 'delta'],\n",
    "    'exp_4': ['a', 'b', 'c', 'alpha'],\n",
    "    'mmf': ['alpha', 'beta', 'k', 'delta'],\n",
    "    'pow_4': ['a', 'b', 'c', 'alpha'],\n",
    "    'log_power': ['a', 'b', 'c'],\n",
    "    'hill_3': ['eta', 'k', 'theta', 'alpha'],\n",
    "    'log_log_linear': ['a', 'b'],\n",
    "    'pow_3': ['a', 'c', 'alpha'],\n",
    "    'vapor_pressure': ['a', 'b', 'c'], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default_values = {\n",
    "    'linear': [0.5, 0.5],\n",
    "    'ilog2': [0.40999, 0.78],\n",
    "    'weibull': [.7, 0.1, 0.01, 1],\n",
    "    'janoschek': [0.73, 0.07,  0.355,  0.46],\n",
    "    'exp_4': [0.8, -0.8,0.7, 0.3],\n",
    "    'mmf': [.7,  0.1, 0.01,  5],\n",
    "    'pow_4': [200, 0., 1.001,  0.1],\n",
    "    'log_power': [0.77,  2.98, -0.51],\n",
    "    'hill_3': [0.586449, 2.460843, 0.772320, 0.1],\n",
    "    'log_log_linear': [1.0, 1.00001],\n",
    "    'pow_3':[0.52, 0.84, 0.01],\n",
    "    'vapor_pressure': [-0.622028, -0.470050,  0.042322],  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_values = {\n",
    "    'linear': [0.00001, 0.0001],\n",
    "    'ilog2': [None, 0.6],\n",
    "    'weibull': [None,None, 1e-10, None],\n",
    "    'janoschek': [0.000001, 0.000001, None, None],\n",
    "    'exp_4': [0.5, None,1.0, None],\n",
    "    'mmf': [None, None, 1e-10, None],\n",
    "    'pow_4': [None, 0., 1.000001, 1e-10],\n",
    "    'log_power': [0.0000001, None, None],\n",
    "    'hill_3': [-10.,1e-10, 1e-10, 1e-10],\n",
    "    'log_log_linear': [1e-10,1.000001],\n",
    "    'pow_3':[None, 0.60, None],\n",
    "    'vapor_pressure': None,  \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_values = {\n",
    "    'linear': None,\n",
    "    'ilog2': [0.41, None],\n",
    "    'weibull': None,\n",
    "    'janoschek': None,\n",
    "    'exp_4':  [None, 0.3,None, 1.5],\n",
    "    'mmf': None,\n",
    "    'pow_4': None,\n",
    "    'log_power': None,\n",
    "    'hill_3': [10.,None, 100., None],\n",
    "    'log_log_linear':None,\n",
    "    'pow_3':[0.59999,None , None],\n",
    "    'vapor_pressure': None,  \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_parameters = 0\n",
    "for f in list_functions:\n",
    "    n_parameters += len(parameters_functions[f])\n",
    "n_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_parameters+12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear',\n",
       " 'hill_3',\n",
       " 'vapor_pressure',\n",
       " 'mmf',\n",
       " 'janoschek',\n",
       " 'log_power',\n",
       " 'log_log_linear',\n",
       " 'weibull',\n",
       " 'exp_4',\n",
       " 'pow_4',\n",
       " 'pow_3',\n",
       " 'ilog2']"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_combination(x, weights, params):\n",
    "    \"\"\"\n",
    "    weights: [float]\n",
    "    params: [float], they are in the order given by list_functions\n",
    "    \"\"\"\n",
    "    copy_= list(params)\n",
    "    val = 0.0\n",
    "    \n",
    "    \n",
    "    for index_f, funct in enumerate(list_functions):\n",
    "        function = functions[funct]\n",
    "        parameters = parameters_functions[funct]\n",
    "        \n",
    "        n_params = len(parameters)\n",
    "        par = params[0: n_params]\n",
    "        params = params[n_params:]\n",
    "        \n",
    "        param_tmp = {}\n",
    "   \n",
    "        for index, p in enumerate(parameters):\n",
    "            \n",
    "            param_tmp[p] = par[index]\n",
    "        val += weights[index_f] * function(x, **param_tmp)\n",
    "        if val!=val:\n",
    "            print function\n",
    "            print x, param_tmp\n",
    "            df\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.677419636671692"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.ones(12)\n",
    "par = np.ones(n_parameters)\n",
    "weighted_combination(1, w, par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior and posterior.\n",
    "### I'm including sqrt(n) in the mean. So, Ln|Xn-X*| follws a gp with mean (1/sqrt(n))(-log(f(n, params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_iterations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood_mean(self, parameters_kernel, params_mean, weights):\n",
    "    \"\"\"\n",
    "    GP log likelihood: y(x) ~ f(x) + epsilon, where epsilon(x) are iid N(0,var_noise), and\n",
    "    f(x) ~ GP(mean, cov)\n",
    "\n",
    "    :param var_noise: (float) variance of the noise\n",
    "    :param mean: (float)\n",
    "    :param parameters_kernel: np.array(k), The order of the parameters is given in the\n",
    "        definition of the class kernel.\n",
    "    :return: float\n",
    "\n",
    "    \"\"\"\n",
    "    f = lambda r: np.sqrt(float(r))\n",
    "    X_data = self.data['points']\n",
    "    n = len(X_data)\n",
    "    cov = covariance_diff_kernel(self, X_data, parameters_kernel)\n",
    "    chol = cholesky(cov,  max_tries=7)\n",
    "    \n",
    "    mean_vector = np.zeros(len(X_data))\n",
    "    for i in range(len(mean_vector)):\n",
    "        mean_ = -np.log(weighted_combination(i+1,weights, params_mean)) / f(i+1)\n",
    "        mean_ -= -np.log(weighted_combination(i+2,weights, params_mean)) / f(i+2)\n",
    "        mean_vector[i] = mean_\n",
    "\n",
    "    y_unbiased = self.data['evaluations'] - mean_vector\n",
    "    solve = cho_solve(chol, y_unbiased)\n",
    "\n",
    "    return -np.sum(np.log(np.diag(chol))) - 0.5 * np.dot(y_unbiased, solve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_prob_with_mean(x,self, total_iterations=total_iterations,lower=np.exp(-1.0), upper=1.04):\n",
    "    \"\"\"\n",
    "    log_prob for only one starting point\n",
    "    Historical data of only one starting point.\n",
    "    \n",
    "    x[0:kernel_params] are the kenrel params\n",
    "    then we have the weights, and then the parameters of te functions\n",
    "    historical_data: [float]\n",
    "    \"\"\"\n",
    "   \n",
    "    bounds = []\n",
    "   \n",
    "#     print \"params\"\n",
    "#     print x\n",
    "    index = 0\n",
    "    for f in list_functions:\n",
    "        if min_values[f] is not None or max_values[f] is not None:\n",
    "            bd = []\n",
    "            if min_values[f] is not None:\n",
    "                n_bounds = len(min_values[f])\n",
    "            else:\n",
    "                n_bounds = len(max_values[f])\n",
    "            \n",
    "            for i in range(n_bounds):\n",
    "                bd_ = [None, None]\n",
    "                if min_values[f] is not None:\n",
    "                    bd_[0] =  min_values[f][i]\n",
    "                if max_values[f] is not None:\n",
    "                    bd_[1] = max_values[f][i]\n",
    "                bd += [bd_]\n",
    "            bounds += bd\n",
    "            \n",
    "        else:\n",
    "            bd = len(parameters_functions[f]) * [[None, None]]\n",
    "            bounds += bd\n",
    "            \n",
    "                    \n",
    "    n_functions = len(list_functions)\n",
    "    \n",
    "    dim_kernel_params = self.dimension_parameters\n",
    "    kernel_params = x[0:dim_kernel_params]\n",
    "    weights = np.array(x[dim_kernel_params:n_functions+dim_kernel_params])\n",
    "    \n",
    "    if np.any(weights <= 0):\n",
    "      #  print \"weights\"\n",
    "        return -np.inf\n",
    "    weights = weights / np.sum(weights)\n",
    "    params =  x[n_functions+dim_kernel_params:]\n",
    "    \n",
    "   \n",
    "    for i in range(n_functions+dim_kernel_params, len(x)):       \n",
    "        if bounds[i - n_functions -dim_kernel_params][0] is not None and x[i] < bounds[i - n_functions -dim_kernel_params][0]:\n",
    "#             print \"wrong_bo\"\n",
    "            return -np.inf\n",
    "        if bounds[i - n_functions - dim_kernel_params][1] is not None and x[i] > bounds[i - n_functions -dim_kernel_params][1]:\n",
    "#             print \"wrong_bo\"\n",
    "            return -np.inf\n",
    "    if np.any(weights <= 0):\n",
    "#         print \"negative weights\"\n",
    "        return -np.inf\n",
    "\n",
    "    if weighted_combination(1,weights, params) >= weighted_combination(total_iterations,weights, params):\n",
    "#         print \"no incease\"\n",
    "        return -np.inf\n",
    "    \n",
    "        \n",
    "    if lower is not None and weighted_combination(1,weights, params) < lower:\n",
    "       # print \"no lower\"\n",
    "        return - np.inf\n",
    "    if upper is not None and weighted_combination(total_iterations,weights, params) > upper:\n",
    "       # print \"no upper\"\n",
    "       # print weighted_combination(total_iterations,weights, params)\n",
    "#         print weighted_combination(1,weights, params)\n",
    "#         print weights\n",
    "        return -np.inf\n",
    "    \n",
    "    lp = 0.0\n",
    "    parameters_model = self.get_parameters_model[2:]\n",
    "    index = 0\n",
    "\n",
    "    for parameter in parameters_model:\n",
    "        dimension = parameter.dimension\n",
    "        lp += parameter.log_prior(kernel_params[index: index + dimension])\n",
    "        index += dimension\n",
    "\n",
    "    if not np.isinf(lp):\n",
    "        lp += log_likelihood_mean(self, kernel_params, params, weights)\n",
    "#     print \"llllh\"\n",
    "#     print lp\n",
    "#     print kernel_params, params, weights\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.ones(12)\n",
    "par = np.ones(n_parameters)\n",
    "params_kernel = gp_model.start_point_sampler\n",
    "x_params = np.concatenate((w, par, params_kernel))\n",
    "log_prob_with_mean(x_params, gp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_prob_per_function(x, historical_data, function_name, weight=1.0):\n",
    "    \"\"\"\n",
    "    log_prob for only one starting point\n",
    "    Historical data of only one starting point.\n",
    "    \n",
    "    x: dictionary with arguments of function\n",
    "    historical_data: [float]\n",
    "    \"\"\"\n",
    "    f = lambda r: np.sqrt(float(r))\n",
    "    \n",
    "    function = functions[function_name]\n",
    "    params = x\n",
    "    n_iterations = len(historical_data)\n",
    "    params_names = parameters_functions[function_name]  \n",
    "    \n",
    "    \n",
    "    dom_x = range(1, len(historical_data) + 1)\n",
    "    evaluations = np.zeros(len(historical_data))\n",
    "    for i in dom_x:\n",
    "        first_1 = weight * function(i, **params)\n",
    "        first_2 = weight * function(i+1, **params)\n",
    "        val = -np.log(first_1) / f(i)\n",
    "        val -= -np.log(first_2) / f(i+1)\n",
    "        evaluations[i-1] =  val\n",
    "    \n",
    "    val = -1.0 * np.sum((evaluations - historical_data) ** 2) \n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_llh_per_function(x, historical_data, function_name, weight=1.0):\n",
    "    \"\"\"\n",
    "    Gradient of the llh respect to a specific function.\n",
    "    \n",
    "    :param function: str\n",
    "    \"\"\"\n",
    "    f = lambda r: np.sqrt(float(r))\n",
    "    function = functions[function_name]\n",
    "    gradient_function = gradients_functions[function_name]\n",
    "    params = x\n",
    "    n_iterations = len(historical_data)\n",
    "    \n",
    "    bounds = []\n",
    "    \n",
    "    evaluations = np.zeros(len(historical_data))\n",
    "    \n",
    "    dom_x = range(1, len(historical_data) + 1)\n",
    "    gradient_theta = np.zeros(len(x))\n",
    "    for i in dom_x:     \n",
    "        evaluations[i-1] = -np.log(weight * function(i, **params)) / f(i)\n",
    "        evaluations[i-1] -= -np.log(weight * function(i+1, **params)) / f(i+1)\n",
    "        tmp = - weight * gradient_function(i, **params) / (f(i) * weight * function(i, **params))\n",
    "        tmp -= -weight * gradient_function(i+1, **params) / (f(i+1) * weight * function(i+1, **params))\n",
    "        gradient_theta += tmp *  (evaluations[i-1] - historical_data[i-1])\n",
    "        \n",
    "    gradient_theta *= -2.0\n",
    "\n",
    "    return gradient_theta\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [ 0.52 , 0.84 , 0.01]\n",
    "historical_data = gp_model.data['evaluations']\n",
    "function_name = 'pow_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.52, 'alpha': 0.01, 'c': 0.84}"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_tmp = {}\n",
    "params_names = parameters_functions[function_name]\n",
    "for ind, name in enumerate(params_names):\n",
    "    param_tmp[name] = x[ind]\n",
    "\n",
    "param_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59802425, -0.58703124,  0.57078713])"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_llh_per_function(param_tmp, historical_data, function_name, weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original = log_prob_per_function(param_tmp, historical_data, function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.587055368015\n"
     ]
    }
   ],
   "source": [
    "dh = 0.0001\n",
    "x = [ 0.52  , 0.84, 0.01 ]\n",
    "x[1] +=dh\n",
    "param_tmp = {}\n",
    "params_names = parameters_functions[function_name]\n",
    "for ind, name in enumerate(params_names):\n",
    "    param_tmp[name] = x[ind]\n",
    "\n",
    "new = log_prob_per_function(param_tmp, historical_data, function_name)\n",
    "print (new - original) / dh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Estimator for parametric mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit, leastsq, fmin_bfgs, fmin_l_bfgs_b, nnls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mle_params_per_function(historical_data, function_name, lower=np.exp(-1.0), upper=1.04, total_iterations=20):\n",
    "    \"\"\"\n",
    "    log_prob for only one starting point\n",
    "    \n",
    "    :param historical_data: [float] \n",
    "    \"\"\"\n",
    "    historical_data = np.array(historical_data)\n",
    "    n = len(historical_data)\n",
    "    x = range(1, n + 1)\n",
    "    function = functions[function_name]\n",
    "    def objective(params):\n",
    "        param_tmp = {}\n",
    "        params_names = parameters_functions[function_name]\n",
    "        for ind, name in enumerate(params_names):\n",
    "            param_tmp[name] = params[ind]\n",
    "        args_lp = list(params)\n",
    "        val = log_prob_per_function(param_tmp, historical_data, function_name)\n",
    "        return -1.0 * val\n",
    "\n",
    "    def gradient(params):\n",
    "        param_tmp = {}\n",
    "        params_names = parameters_functions[function_name]\n",
    "        for ind, name in enumerate(params_names):\n",
    "            param_tmp[name] = params[ind]     \n",
    "        val = gradient_llh_per_function(param_tmp, historical_data, function_name)\n",
    "        \n",
    "        for t in val:\n",
    "            if t!=t:\n",
    "                print \"grad\"\n",
    "                print param_tmp\n",
    "                print params\n",
    "                print function_name\n",
    "                df\n",
    "        return -1.0 * val\n",
    "    \n",
    "    n_params_function = len(parameters_functions[function_name])\n",
    "    \n",
    "    params_st = np.ones(n_params_function)\n",
    "    \n",
    "    bounds = []\n",
    "    \n",
    "    if default_values[function_name] is not None:\n",
    "        params_st[0:len(default_values[function_name])] = default_values[function_name]\n",
    "        \n",
    "        \n",
    "    if min_values[function_name] is not None:\n",
    "        bd = []\n",
    "        for t in min_values[function_name]:\n",
    "            bd += [[t, None]]\n",
    "        bounds += bd\n",
    " \n",
    "        \n",
    "    if max_values[function_name] is not None:\n",
    "        bd = []\n",
    "        for it, t in enumerate(max_values[function_name]):\n",
    "            if t is not None and len(bounds) > 0:\n",
    "                bounds[it][1] = t\n",
    "            elif len(bounds) == 0:\n",
    "                bd += [[None, t]]\n",
    "            \n",
    "        if len(bounds) == 0:\n",
    "            bounds += bd\n",
    "          \n",
    "    if max_values[function_name] is None and min_values[function_name] is None:\n",
    "        bounds = len(default_values[function_name]) * [[None, None]]\n",
    "\n",
    "    \n",
    "    popt, fval, info= fmin_l_bfgs_b(objective,\n",
    "                                    fprime=gradient,\n",
    "                                x0=params_st,\n",
    "                                bounds=bounds,\n",
    "                                approx_grad=False)\n",
    "    weight = 1.0\n",
    "    param_tmp = {}\n",
    "    params_names = parameters_functions[function_name]\n",
    "    for ind, name in enumerate(params_names):\n",
    "        param_tmp[name] = popt[ind]\n",
    "        \n",
    "    if lower is not None and function(1, **param_tmp) < lower:\n",
    "        weight = lower / function(1, **param_tmp)\n",
    "        \n",
    "    if upper is not None and function(total_iterations,**param_tmp) > upper:\n",
    "        weight = upper / function(total_iterations,**param_tmp)\n",
    "\n",
    "    \n",
    "    return popt, fval, info, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_starting_values(historical_data):\n",
    "    st_weights=[]\n",
    "    st_params=[]\n",
    "    for function_n in list_functions:\n",
    "        params_names = parameters_functions[function_n]\n",
    "        result = mle_params_per_function(historical_data, function_n)\n",
    "        st_params += list(result[0])\n",
    "        st_weights.append(result[-1])\n",
    "    st_weights = [t / np.sum(st_weights) for t in st_weights]\n",
    "    start_params = st_weights + list(st_params) \n",
    "    start_params = np.array(start_params)\n",
    "    \n",
    "    return start_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ## Slice Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_parameters_mean(self, n_samples, start_point=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    Sample parameters of the model from the posterior without considering burning.\n",
    "\n",
    "    :param n_samples: (int)\n",
    "    :param start_point: np.array(n_parameters)\n",
    "    :param random_seed: int\n",
    "\n",
    "    :return: n_samples * [np.array(float)]\n",
    "    \"\"\"\n",
    "\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    if start_point is None:\n",
    "        start_point = self.samples_parameters[-1]\n",
    "\n",
    "\n",
    "    n_samples *= (self.thinning + 1)\n",
    "    n_samples = int(n_samples)\n",
    "\n",
    "\n",
    "\n",
    "    for sample in range(n_samples):\n",
    "        print \"sample!!!!!\"\n",
    "        print sample\n",
    "        points = separate_vector(start_point, range(self.dimension_parameters))\n",
    "        for index, slice in enumerate(self.slice_samplers):\n",
    "            new_point_ = None\n",
    "            n_try = 0\n",
    "            while new_point_ is None and n_try < 10:\n",
    "                new_point_ = \\\n",
    "                    slice.slice_sample(points[index], points[1 - index], *(self, ))\n",
    "                try:\n",
    "                    new_point_ = \\\n",
    "                        slice.slice_sample(points[index], points[1 - index], *(self, ))\n",
    "                except Exception as e:\n",
    "                    n_try += 1\n",
    "                    new_point_ = None\n",
    "            if new_point_ is None:\n",
    "                logger.info('program failed to compute a sample of the parameters')\n",
    "                sys.exit(1)\n",
    "            points[index] = new_point_\n",
    "        start_point = combine_vectors(points[0], points[1], range(self.dimension_parameters))\n",
    "        samples.append(start_point)\n",
    "    samples_return = samples[::self.thinning + 1]\n",
    "    self.samples_parameters += samples_return\n",
    "\n",
    "    return samples_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_samplers_mean(self):\n",
    "    \"\"\"\n",
    "    Defines the samplers of the parameters of the model.\n",
    "    We assume that we only have one set of length scale parameters.\n",
    "    \"\"\"\n",
    "    self.slice_samplers = []\n",
    "    self.samples_parameters = []\n",
    "    self.start_point_sampler = []\n",
    "    if self.length_scale_indexes is None:\n",
    "        ignore_index = None\n",
    "\n",
    "        slice_parameters = {\n",
    "            'max_steps_out': self.max_steps_out,\n",
    "            'component_wise': True,\n",
    "        }\n",
    "        self.slice_samplers.append(SliceSampling(\n",
    "            log_prob_with_mean, range(self.dimension_parameters),  ignore_index=ignore_index,\n",
    "            **slice_parameters))\n",
    "        slice_parameters = {\n",
    "            'max_steps_out': self.max_steps_out,\n",
    "            'component_wise': False,\n",
    "        }\n",
    "        self.slice_samplers.append(SliceSampling(\n",
    "            log_prob_with_mean, range(self.dimension_parameters, self.dimension_parameters+ n_parameters+len(list_functions)),  ignore_index=ignore_index,\n",
    "            **slice_parameters))\n",
    "        \n",
    "  \n",
    "\n",
    "    if self.start_point_sampler is not None and len(self.start_point_sampler) > 0:\n",
    "        if len(self.samples_parameters) == 0:\n",
    "            self.samples_parameters.append(np.array(self.start_point_sampler))\n",
    "    else:\n",
    "        self.samples_parameters = []\n",
    "        z = self.get_value_parameters_model\n",
    "        z = z[2:]\n",
    "        mean_params = self.mean_params\n",
    "        self.samples_parameters.append(np.concatenate((z, mean_params)))\n",
    "        if self.n_burning > 0:\n",
    "            parameters = sample_parameters_mean(self, float(self.n_burning) / (self.thinning + 1))\n",
    "            self.samples_parameters = []\n",
    "            self.samples_parameters.append(parameters[-1])\n",
    "            self.start_point_sampler = parameters[-1]\n",
    "        else:\n",
    "            self.start_point_sampler = self.get_value_parameters_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:stratified_bayesian_optimization.util.json_file:Loading data/gp_models/logistic/gp_GPFittingGaussian_logistic_Ornstein_kernel_default_training_data_10_points_rs_1.json\n",
      "INFO:stratified_bayesian_optimization.services.gp_fitting:Training GPFittingGaussian\n",
      "INFO:stratified_bayesian_optimization.util.json_file:Loading data/gp_models/logistic/gp_GPFittingGaussian_logistic_Ornstein_kernel_default_training_data_10_points_rs_1.json\n",
      "INFO:stratified_bayesian_optimization.services.gp_fitting:Training GPFittingGaussian\n",
      "INFO:stratified_bayesian_optimization.util.json_file:Loading data/gp_models/logistic/gp_GPFittingGaussian_logistic_Ornstein_kernel_default_training_data_10_points_rs_1.json\n",
      "INFO:stratified_bayesian_optimization.services.gp_fitting:Training GPFittingGaussian\n"
     ]
    }
   ],
   "source": [
    "gp_models_mean = {}\n",
    "for i in points:\n",
    "    gp_models_mean[i] = create_gp_model_from_spec(specs[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in points:\n",
    "    st = get_starting_values(gp_models_mean[i].data['evaluations'])\n",
    "    gp_models_mean[i].mean_params = st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(sys.modules['stratified_bayesian_optimization.samplers.slice_sampling'])\n",
    "from stratified_bayesian_optimization.samplers.slice_sampling import SliceSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample!!!!!\n",
      "0\n",
      "sample!!!!!\n",
      "1\n",
      "sample!!!!!\n",
      "2\n",
      "sample!!!!!\n",
      "3\n",
      "sample!!!!!\n",
      "4\n",
      "sample!!!!!\n",
      "5\n",
      "sample!!!!!\n",
      "6\n",
      "sample!!!!!\n",
      "7\n",
      "sample!!!!!\n",
      "8\n",
      "sample!!!!!\n",
      "9\n",
      "sample!!!!!\n",
      "10\n",
      "sample!!!!!\n",
      "11\n",
      "sample!!!!!\n",
      "12\n",
      "sample!!!!!\n",
      "13\n",
      "sample!!!!!\n",
      "14\n",
      "sample!!!!!\n",
      "15\n",
      "sample!!!!!\n",
      "16\n",
      "sample!!!!!\n",
      "17\n",
      "sample!!!!!\n",
      "18\n",
      "sample!!!!!\n",
      "19\n",
      "sample!!!!!\n",
      "20\n",
      "sample!!!!!\n",
      "21\n",
      "sample!!!!!\n",
      "22\n",
      "sample!!!!!\n",
      "23\n",
      "sample!!!!!\n",
      "24\n",
      "sample!!!!!\n",
      "25\n",
      "sample!!!!!\n",
      "26\n",
      "sample!!!!!\n",
      "27\n",
      "sample!!!!!\n",
      "28\n",
      "sample!!!!!\n",
      "29\n",
      "sample!!!!!\n",
      "30\n",
      "sample!!!!!\n",
      "31\n",
      "sample!!!!!\n",
      "32\n",
      "sample!!!!!\n",
      "33\n",
      "sample!!!!!\n",
      "34\n",
      "sample!!!!!\n",
      "35\n",
      "sample!!!!!\n",
      "36\n",
      "sample!!!!!\n",
      "37\n",
      "sample!!!!!\n",
      "38\n",
      "sample!!!!!\n",
      "39\n",
      "sample!!!!!\n",
      "40\n",
      "sample!!!!!\n",
      "41\n",
      "sample!!!!!\n",
      "42\n",
      "sample!!!!!\n",
      "43\n",
      "sample!!!!!\n",
      "44\n",
      "sample!!!!!\n",
      "45\n",
      "sample!!!!!\n",
      "46\n",
      "sample!!!!!\n",
      "47\n",
      "sample!!!!!\n",
      "48\n",
      "sample!!!!!\n",
      "49\n",
      "sample!!!!!\n",
      "50\n",
      "sample!!!!!\n",
      "51\n",
      "sample!!!!!\n",
      "52\n",
      "sample!!!!!\n",
      "53\n",
      "sample!!!!!\n",
      "54\n",
      "sample!!!!!\n",
      "55\n",
      "sample!!!!!\n",
      "56\n",
      "sample!!!!!\n",
      "57\n",
      "sample!!!!!\n",
      "58\n",
      "sample!!!!!\n",
      "59\n",
      "sample!!!!!\n",
      "60\n",
      "sample!!!!!\n",
      "61\n",
      "sample!!!!!\n",
      "62\n",
      "sample!!!!!\n",
      "63\n",
      "sample!!!!!\n",
      "64\n",
      "sample!!!!!\n",
      "65\n",
      "sample!!!!!\n",
      "66\n",
      "sample!!!!!\n",
      "67\n",
      "sample!!!!!\n",
      "68\n",
      "sample!!!!!\n",
      "69\n",
      "sample!!!!!\n",
      "70\n",
      "sample!!!!!\n",
      "71\n",
      "sample!!!!!\n",
      "72\n",
      "sample!!!!!\n",
      "73\n",
      "sample!!!!!\n",
      "74\n",
      "sample!!!!!\n",
      "75\n",
      "sample!!!!!\n",
      "76\n",
      "sample!!!!!\n",
      "77\n",
      "sample!!!!!\n",
      "78\n",
      "sample!!!!!\n",
      "79\n",
      "sample!!!!!\n",
      "80\n",
      "sample!!!!!\n",
      "81\n",
      "sample!!!!!\n",
      "82\n",
      "sample!!!!!\n",
      "83\n",
      "sample!!!!!\n",
      "84\n",
      "sample!!!!!\n",
      "85\n",
      "sample!!!!!\n",
      "86\n",
      "sample!!!!!\n",
      "87\n",
      "sample!!!!!\n",
      "88\n",
      "sample!!!!!\n",
      "89\n",
      "sample!!!!!\n",
      "90\n",
      "sample!!!!!\n",
      "91\n",
      "sample!!!!!\n",
      "92\n",
      "sample!!!!!\n",
      "93\n",
      "sample!!!!!\n",
      "94\n",
      "sample!!!!!\n",
      "95\n",
      "sample!!!!!\n",
      "96\n",
      "sample!!!!!\n",
      "97\n",
      "sample!!!!!\n",
      "98\n",
      "sample!!!!!\n",
      "99\n",
      "sample!!!!!\n",
      "100\n",
      "sample!!!!!\n",
      "101\n",
      "sample!!!!!\n",
      "102\n",
      "sample!!!!!\n",
      "103\n",
      "sample!!!!!\n",
      "104\n",
      "sample!!!!!\n",
      "105\n",
      "sample!!!!!\n",
      "106\n",
      "sample!!!!!\n",
      "107\n",
      "sample!!!!!\n",
      "108\n",
      "sample!!!!!\n",
      "109\n",
      "sample!!!!!\n",
      "110\n",
      "sample!!!!!\n",
      "111\n",
      "sample!!!!!\n",
      "112\n",
      "sample!!!!!\n",
      "113\n",
      "sample!!!!!\n",
      "114\n",
      "sample!!!!!\n",
      "115\n",
      "sample!!!!!\n",
      "116\n",
      "sample!!!!!\n",
      "117\n",
      "sample!!!!!\n",
      "118\n",
      "sample!!!!!\n",
      "119\n",
      "sample!!!!!\n",
      "120\n",
      "sample!!!!!\n",
      "121\n",
      "sample!!!!!\n",
      "122\n",
      "sample!!!!!\n",
      "123\n",
      "sample!!!!!\n",
      "124\n",
      "sample!!!!!\n",
      "125\n",
      "sample!!!!!\n",
      "126\n",
      "sample!!!!!\n",
      "127\n",
      "sample!!!!!\n",
      "128\n",
      "sample!!!!!\n",
      "129\n",
      "sample!!!!!\n",
      "130\n",
      "sample!!!!!\n",
      "131\n",
      "sample!!!!!\n",
      "132\n",
      "sample!!!!!\n",
      "133\n",
      "sample!!!!!\n",
      "134\n",
      "sample!!!!!\n",
      "135\n",
      "sample!!!!!\n",
      "136\n",
      "sample!!!!!\n",
      "137\n",
      "sample!!!!!\n",
      "138\n",
      "sample!!!!!\n",
      "139\n",
      "sample!!!!!\n",
      "140\n",
      "sample!!!!!\n",
      "141\n",
      "sample!!!!!\n",
      "142\n",
      "sample!!!!!\n",
      "143\n",
      "sample!!!!!\n",
      "144\n",
      "sample!!!!!\n",
      "145\n",
      "sample!!!!!\n",
      "146\n",
      "sample!!!!!\n",
      "147\n",
      "sample!!!!!\n",
      "148\n",
      "sample!!!!!\n",
      "149\n",
      "sample!!!!!\n",
      "150\n",
      "sample!!!!!\n",
      "151\n",
      "sample!!!!!\n",
      "152\n",
      "sample!!!!!\n",
      "153\n",
      "sample!!!!!\n",
      "154\n",
      "sample!!!!!\n",
      "155\n",
      "sample!!!!!\n",
      "156\n",
      "sample!!!!!\n",
      "157\n",
      "sample!!!!!\n",
      "158\n",
      "sample!!!!!\n",
      "159\n",
      "sample!!!!!\n",
      "160\n",
      "sample!!!!!\n",
      "161\n",
      "sample!!!!!\n",
      "162\n",
      "sample!!!!!\n",
      "163\n",
      "sample!!!!!\n",
      "164\n",
      "sample!!!!!\n",
      "165\n",
      "sample!!!!!\n",
      "166\n",
      "sample!!!!!\n",
      "167\n",
      "sample!!!!!\n",
      "168\n",
      "sample!!!!!\n",
      "169\n",
      "sample!!!!!\n",
      "170\n",
      "sample!!!!!\n",
      "171\n",
      "sample!!!!!\n",
      "172\n",
      "sample!!!!!\n",
      "173\n",
      "sample!!!!!\n",
      "174\n",
      "sample!!!!!\n",
      "175\n",
      "sample!!!!!\n",
      "176\n",
      "sample!!!!!\n",
      "177\n",
      "sample!!!!!\n",
      "178\n",
      "sample!!!!!\n",
      "179\n",
      "sample!!!!!\n",
      "180\n",
      "sample!!!!!\n",
      "181\n",
      "sample!!!!!\n",
      "182\n",
      "sample!!!!!\n",
      "183\n",
      "sample!!!!!\n",
      "184\n",
      "sample!!!!!\n",
      "185\n",
      "sample!!!!!\n",
      "186\n",
      "sample!!!!!\n",
      "187\n",
      "sample!!!!!\n",
      "188\n",
      "sample!!!!!\n",
      "189\n",
      "sample!!!!!\n",
      "190\n",
      "sample!!!!!\n",
      "191\n",
      "sample!!!!!\n",
      "192\n",
      "sample!!!!!\n",
      "193\n",
      "sample!!!!!\n",
      "194\n",
      "sample!!!!!\n",
      "195\n",
      "sample!!!!!\n",
      "196\n",
      "sample!!!!!\n",
      "197\n",
      "sample!!!!!\n",
      "198\n",
      "sample!!!!!\n",
      "199\n",
      "sample!!!!!\n",
      "200\n",
      "sample!!!!!\n",
      "201\n",
      "sample!!!!!\n",
      "202\n",
      "sample!!!!!\n",
      "203\n",
      "sample!!!!!\n",
      "204\n",
      "sample!!!!!\n",
      "205\n",
      "sample!!!!!\n",
      "206\n",
      "sample!!!!!\n",
      "207\n",
      "sample!!!!!\n",
      "208\n",
      "sample!!!!!\n",
      "209\n",
      "sample!!!!!\n",
      "210\n",
      "sample!!!!!\n",
      "211\n",
      "sample!!!!!\n",
      "212\n",
      "sample!!!!!\n",
      "213\n",
      "sample!!!!!\n",
      "214\n",
      "sample!!!!!\n",
      "215\n",
      "sample!!!!!\n",
      "216\n",
      "sample!!!!!\n",
      "217\n",
      "sample!!!!!\n",
      "218\n",
      "sample!!!!!\n",
      "219\n",
      "sample!!!!!\n",
      "220\n",
      "sample!!!!!\n",
      "221\n",
      "sample!!!!!\n",
      "222\n",
      "sample!!!!!\n",
      "223\n",
      "sample!!!!!\n",
      "224\n",
      "sample!!!!!\n",
      "225\n",
      "sample!!!!!\n",
      "226\n",
      "sample!!!!!\n",
      "227\n",
      "sample!!!!!\n",
      "228\n",
      "sample!!!!!\n",
      "229\n",
      "sample!!!!!\n",
      "230\n",
      "sample!!!!!\n",
      "231\n",
      "sample!!!!!\n",
      "232\n",
      "sample!!!!!\n",
      "233\n",
      "sample!!!!!\n",
      "234\n",
      "sample!!!!!\n",
      "235\n",
      "sample!!!!!\n",
      "236\n",
      "sample!!!!!\n",
      "237\n",
      "sample!!!!!\n",
      "238\n",
      "sample!!!!!\n",
      "239\n",
      "sample!!!!!\n",
      "240\n",
      "sample!!!!!\n",
      "241\n",
      "sample!!!!!\n",
      "242\n",
      "sample!!!!!\n",
      "243\n",
      "sample!!!!!\n",
      "244\n",
      "sample!!!!!\n",
      "245\n",
      "sample!!!!!\n",
      "246\n",
      "sample!!!!!\n",
      "247\n",
      "sample!!!!!\n",
      "248\n",
      "sample!!!!!\n",
      "249\n",
      "sample!!!!!\n",
      "250\n",
      "sample!!!!!\n",
      "251\n",
      "sample!!!!!\n",
      "252\n",
      "sample!!!!!\n",
      "253\n",
      "sample!!!!!\n",
      "254\n",
      "sample!!!!!\n",
      "255\n",
      "sample!!!!!\n",
      "256\n",
      "sample!!!!!\n",
      "257\n",
      "sample!!!!!\n",
      "258\n",
      "sample!!!!!\n",
      "259\n",
      "sample!!!!!\n",
      "260\n",
      "sample!!!!!\n",
      "261\n",
      "sample!!!!!\n",
      "262\n",
      "sample!!!!!\n",
      "263\n",
      "sample!!!!!\n",
      "264\n",
      "sample!!!!!\n",
      "265\n",
      "sample!!!!!\n",
      "266\n",
      "sample!!!!!\n",
      "267\n",
      "sample!!!!!\n",
      "268\n",
      "sample!!!!!\n",
      "269\n",
      "sample!!!!!\n",
      "270\n",
      "sample!!!!!\n",
      "271\n",
      "sample!!!!!\n",
      "272\n",
      "sample!!!!!\n",
      "273\n",
      "sample!!!!!\n",
      "274\n",
      "sample!!!!!\n",
      "275\n",
      "sample!!!!!\n",
      "276\n",
      "sample!!!!!\n",
      "277\n",
      "sample!!!!!\n",
      "278\n",
      "sample!!!!!\n",
      "279\n",
      "sample!!!!!\n",
      "280\n",
      "sample!!!!!\n",
      "281\n",
      "sample!!!!!\n",
      "282\n",
      "sample!!!!!\n",
      "283\n",
      "sample!!!!!\n",
      "284\n",
      "sample!!!!!\n",
      "285\n",
      "sample!!!!!\n",
      "286\n",
      "sample!!!!!\n",
      "287\n",
      "sample!!!!!\n",
      "288\n",
      "sample!!!!!\n",
      "289\n",
      "sample!!!!!\n",
      "290\n",
      "sample!!!!!\n",
      "291\n",
      "sample!!!!!\n",
      "292\n",
      "sample!!!!!\n",
      "293\n",
      "sample!!!!!\n",
      "294\n",
      "sample!!!!!\n",
      "295\n",
      "sample!!!!!\n",
      "296\n",
      "sample!!!!!\n",
      "297\n",
      "sample!!!!!\n",
      "298\n",
      "sample!!!!!\n",
      "299\n",
      "sample!!!!!\n",
      "300\n",
      "sample!!!!!\n",
      "301\n",
      "sample!!!!!\n",
      "302\n",
      "sample!!!!!\n",
      "303\n",
      "sample!!!!!\n",
      "304\n",
      "sample!!!!!\n",
      "305\n",
      "sample!!!!!\n",
      "306\n",
      "sample!!!!!\n",
      "307\n",
      "sample!!!!!\n",
      "308\n",
      "sample!!!!!\n",
      "309\n",
      "sample!!!!!\n",
      "310\n",
      "sample!!!!!\n",
      "311\n",
      "sample!!!!!\n",
      "312\n",
      "sample!!!!!\n",
      "313\n",
      "sample!!!!!\n",
      "314\n",
      "sample!!!!!\n",
      "315\n",
      "sample!!!!!\n",
      "316\n",
      "sample!!!!!\n",
      "317\n",
      "sample!!!!!\n",
      "318\n",
      "sample!!!!!\n",
      "319\n",
      "sample!!!!!\n",
      "320\n",
      "sample!!!!!\n",
      "321\n",
      "sample!!!!!\n",
      "322\n",
      "sample!!!!!\n",
      "323\n",
      "sample!!!!!\n",
      "324\n",
      "sample!!!!!\n",
      "325\n",
      "sample!!!!!\n",
      "326\n",
      "sample!!!!!\n",
      "327\n",
      "sample!!!!!\n",
      "328\n",
      "sample!!!!!\n",
      "329\n",
      "sample!!!!!\n",
      "330\n",
      "sample!!!!!\n",
      "331\n",
      "sample!!!!!\n",
      "332\n",
      "sample!!!!!\n",
      "333\n",
      "sample!!!!!\n",
      "334\n",
      "sample!!!!!\n",
      "335\n",
      "sample!!!!!\n",
      "336\n",
      "sample!!!!!\n",
      "337\n",
      "sample!!!!!\n",
      "338\n",
      "sample!!!!!\n",
      "339\n",
      "sample!!!!!\n",
      "340\n",
      "sample!!!!!\n",
      "341\n",
      "sample!!!!!\n",
      "342\n",
      "sample!!!!!\n",
      "343\n",
      "sample!!!!!\n",
      "344\n",
      "sample!!!!!\n",
      "345\n",
      "sample!!!!!\n",
      "346\n",
      "sample!!!!!\n",
      "347\n",
      "sample!!!!!\n",
      "348\n",
      "sample!!!!!\n",
      "349\n",
      "sample!!!!!\n",
      "350\n",
      "sample!!!!!\n",
      "351\n",
      "sample!!!!!\n",
      "352\n",
      "sample!!!!!\n",
      "353\n",
      "sample!!!!!\n",
      "354\n",
      "sample!!!!!\n",
      "355\n",
      "sample!!!!!\n",
      "356\n",
      "sample!!!!!\n",
      "357\n",
      "sample!!!!!\n",
      "358\n",
      "sample!!!!!\n",
      "359\n",
      "sample!!!!!\n",
      "360\n",
      "sample!!!!!\n",
      "361\n",
      "sample!!!!!\n",
      "362\n",
      "sample!!!!!\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "set_samplers_mean(gp_models_mean[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluations': array([ 0.00133333,  0.0023    ]),\n",
       " 'points': [(1, 2), (2, 3)],\n",
       " 'var_noise': None}"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_models_mean[2].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_posterior_params_with_mean(self, params):\n",
    "    ##Compute posterior parameters of f(x*) including a parametric mean\n",
    "    f = lambda x: np.sqrt(float(x))\n",
    "    current_point = [self.current_iterations]\n",
    "    \n",
    "    \n",
    "    n_functions = len(list_functions)\n",
    "    \n",
    "    dim_kernel_params = self.dimension_parameters\n",
    "    params = x[0:dim_kernel_params]\n",
    "    weights = np.array(x[dim_kernel_params:n_functions+dim_kernel_params])\n",
    "    weights = weights / np.sum(weights)\n",
    "    params_mean =  x[n_functions+dim_kernel_params:]\n",
    "    \n",
    "    X_data = self.data['points']\n",
    "    vector_ = cov_diff_point(self, params, current_point, X_data)\n",
    "    \n",
    "    cov = covariance_diff_kernel(self, X_data, params)\n",
    "    chol = cholesky(cov,  max_tries=7)\n",
    "    \n",
    "    \n",
    "    mean_vector = np.zeros(len(X_data))\n",
    "  \n",
    "    for i in range(len(mean_vector)):\n",
    "        mean_ = -np.log(weighted_combination(i+1,weights, params_mean)) / f(i+1)\n",
    "        mean_ -= -np.log(weighted_combination(i+2,weights, params_mean)) / f(i+2)\n",
    "        mean_vector[i] = mean_\n",
    "    \n",
    "    y_unbiased = self.data['evaluations'] - mean_vector\n",
    "    solve = cho_solve(chol, y_unbiased) \n",
    "    \n",
    "    part_2 = cho_solve(chol, vector_)\n",
    "    \n",
    "    prior_mean = -np.log(weighted_combination(current_point[0],weights, params_mean)) / f(current_point[0])\n",
    "    mean =  self.raw_results[-1] + np.dot(part_2, solve)  + prior_mean\n",
    "    \n",
    "    raw_cov = self.kernel.evaluate_cov_defined_by_params(params, np.array([current_point]), 1) / f(current_point[0]) \n",
    "    \n",
    "    var = raw_cov - np.dot(part_2, part_2)\n",
    "    \n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_posterior_params_marginalize_with_mean(self, n_samples=10, burning_parameters=True):\n",
    "    \n",
    "    if burning_parameters:\n",
    "        parameters = sample_parameters_mean(self, float(self.n_burning) / (self.thinning + 1))\n",
    "        self.samples_parameters = []\n",
    "        self.samples_parameters.append(parameters[-1])\n",
    "        self.start_point_sampler = parameters[-1]\n",
    "        \n",
    "    parameters = sample_parameters_mean(self, n_samples)\n",
    "    \n",
    "    means = []\n",
    "    covs = []\n",
    "    for param in parameters:\n",
    "        mean, cov = compute_posterior_params_with_mean(self, param)\n",
    "        means.append(mean)\n",
    "        covs.append(cov)\n",
    "        \n",
    "    mean = np.mean(means)\n",
    "    std = np.sqrt(np.mean(covs))\n",
    "    \n",
    "    ci = [mean-1.96 * std, mean+ 1.96*std]\n",
    "    \n",
    "    return mean, std, ci\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy with means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_results = {}\n",
    "for j in points:\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]\n",
    "    raw_results[j] = [results[j][i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_mean(self, st_point, iterations=21, start=3):\n",
    "    means = []\n",
    "    cis = []\n",
    "    \n",
    "    \n",
    "    mean, std, ci = compute_posterior_params_marginalize_with_mean(self)\n",
    "    means.append(mean)\n",
    "    cis.append(ci)\n",
    "    \n",
    "    for i in range(start, iterations):\n",
    "        print \"iterations!!!\"\n",
    "        print (i)\n",
    "        self = add_observations(self, i + 1, results[st_point][i], results[st_point][i-1])\n",
    "        mean, std, ci = compute_posterior_params_marginalize_with_mean(self)\n",
    "        print mean, ci\n",
    "        means.append(mean)\n",
    "        cis.append(ci)\n",
    "    return means, cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-898-9e90f80d0626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0maccuracy_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp_models_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-897-7428681ed1df>\u001b[0m in \u001b[0;36maccuracy_mean\u001b[0;34m(self, st_point, iterations, start)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_posterior_params_marginalize_with_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-895-5abdf6dd6499>\u001b[0m in \u001b[0;36mcompute_posterior_params_marginalize_with_mean\u001b[0;34m(self, n_samples, burning_parameters)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mburning_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_parameters_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_burning\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthinning\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-889-1161b20bb341>\u001b[0m in \u001b[0;36msample_parameters_mean\u001b[0;34m(self, n_samples, start_point, random_seed)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstart_point\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mstart_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "results  = accuracy_mean(gp_models_mean[2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(-np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_means(means, cis, original_value, start=3):\n",
    "    plt.figure()\n",
    "    x_lim = len(means)\n",
    "    points = range(start, x_lim + start)\n",
    "    plt.plot(points, means,'b')\n",
    "    plt.plot(points, len(points) * [original_value], 'r')\n",
    "    plt.plot(points, [t[0] for t in cis],'g-')\n",
    "    plt.plot(points, [t[1] for t in cis],'g-')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel('Objective function')\n",
    "    plt.xlabel('Iteration')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
